{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrated Adversarial Learning (Part 2)\n",
    "## Robustness evaluation\n",
    "\n",
    "In this second notebook we test the approach for robustness towards network initialisation, mode bias and noise in 5 repetitions of 9 different data configurations. The baseline is the non-regularised adversarially trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypertunity as ht\n",
    "import numpy as np\n",
    "import scipy.stats as sp_stats\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from cal import data, networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x124abd730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As before, we train the calibration network for a fixed number of iterations for each of the data configurations. We also reuse the tranining code from Part 1 but we adapt it to receive a configuration defining some hyperparameters, which we may change between experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step_cal(xs_t, ys_t, opt_cal, net_cal):\n",
    "    opt_cal.zero_grad()\n",
    "    loss = -torch.mean(net_cal.ll(xs_t, ys_t))\n",
    "    loss.backward()\n",
    "    opt_cal.step()\n",
    "    return loss.detach().cpu().numpy()\n",
    "\n",
    "def train_cal(producer, net_cal, opt_cal, config):\n",
    "    loss_history = []\n",
    "    data = producer.produce(config.train.batch_size_cal)\n",
    "    for i in range(config.train.iters_cal):\n",
    "        xs, ys = next(data)\n",
    "        xs_t = torch.as_tensor(xs).to(device)\n",
    "        ys_t = torch.as_tensor(ys).to(device)\n",
    "        loss = run_step_cal(xs_t, ys_t, opt_cal, net_cal)\n",
    "        loss_history.append(loss)\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step_dis(xs, ys, ys_ref, net_dis, opt_dis):\n",
    "    opt_dis.zero_grad()\n",
    "    loss_dis = -torch.mean(\n",
    "        torch.sum(net_dis.ll([xs, ys], torch.ones(len(ys)).to(device)), -1)\n",
    "        + torch.sum(net_dis.ll([xs, ys_ref], torch.zeros(len(ys_ref)).to(device)), -1)\n",
    "    )\n",
    "    loss_dis.backward()\n",
    "    opt_dis.step()\n",
    "    return loss_dis.detach().cpu().numpy()\n",
    "\n",
    "def run_step_ref(xs, ys, ys_cal, net_ref, net_dis, opt_ref, n_samples=1, lambda_cal=0.0):\n",
    "    opt_ref.zero_grad()\n",
    "    ys_ref = net_ref([xs, ys_cal], n_samples=n_samples)\n",
    "    xs_repeated = torch.repeat_interleave(xs, repeats=ys_ref.shape[0] // xs.shape[0], dim=0)\n",
    "    total_loss = 0\n",
    "    losses_per_type = []\n",
    "    if lambda_cal > 0:\n",
    "        ys_ref_mean = torch.mean(ys_ref.view(-1, n_samples), dim=1, keepdim=True)\n",
    "        loss_kl = 0.5 * torch.mean((ys_ref_mean - ys_cal) ** 2)\n",
    "        total_loss += lambda_cal * loss_kl\n",
    "        losses_per_type.append(loss_kl.detach().cpu().numpy())\n",
    "    ll_dis = net_dis.ll([xs_repeated, ys_ref], torch.ones(len(ys_ref)).to(device))\n",
    "    loss_dis = -torch.mean(torch.sum(ll_dis, -1))\n",
    "    total_loss += loss_dis\n",
    "    losses_per_type.append(loss_dis.detach().cpu().numpy())\n",
    "    total_loss.backward()\n",
    "    opt_ref.step()\n",
    "    return np.array(losses_per_type)\n",
    "\n",
    "def eval_nets(producer, net_cal, net_ref):\n",
    "    data = producer.produce(2000)\n",
    "    xs, ys = next(data)\n",
    "    xs_t = torch.as_tensor(xs).to(device)\n",
    "    with torch.no_grad():\n",
    "        ys_cal = net_cal(xs_t).detach()\n",
    "        ys_ref = net_ref([xs_t, ys_cal]).detach()\n",
    "    ys_ref_ll = producer.ll(xs, ys_ref.cpu().numpy().squeeze())\n",
    "    avg_ll = np.mean(ys_ref_ll)\n",
    "    return avg_ll\n",
    "        \n",
    "def train_dis_ref(producer, net_cal, net_ref, opt_ref, net_dis, opt_dis, config):\n",
    "    loss_dis_history = []\n",
    "    loss_ref_history = []\n",
    "    ll_ref = []\n",
    "    loss_dis = np.inf\n",
    "    loss_ref = np.inf\n",
    "    ys_ref_progress = []\n",
    "    data = producer.produce(config.train.batch_size_adv)\n",
    "    for i in range(config.train.iters_adv):\n",
    "        xs, ys = next(data)\n",
    "        xs_t = torch.as_tensor(xs.reshape(-1, 1)).to(device)\n",
    "        ys_t = torch.as_tensor(ys.reshape(-1, 1)).to(device)\n",
    "        with torch.no_grad():\n",
    "            ys_cal = net_cal(xs_t).detach()\n",
    "            ys_ref = net_ref([xs_t, ys_cal]).detach()\n",
    "        loss_dis = run_step_dis(xs_t, ys_t, ys_ref, net_dis, opt_dis)\n",
    "        loss_dis_history.append(loss_dis)\n",
    "        n_samples = config.model.n_samples_cal if config.model.lambda_cal > 0 else 1\n",
    "        loss_ref = run_step_ref(\n",
    "            xs_t, ys_t, ys_cal, net_ref, net_dis, opt_ref, n_samples, config.model.lambda_cal\n",
    "        )\n",
    "        loss_ref_history.append(loss_ref)\n",
    "        if i % 10 == 0:\n",
    "            avg_ll = eval_nets(producer, net_cal, net_ref)\n",
    "            ll_ref.append(avg_ll)\n",
    "    return loss_dis, loss_ref, ll_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next we define a single entry point for an experiment, depending on the configuration only, which we generate later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(config):\n",
    "    producer = data.Bifurcation(pi=config.data.pi, sigma=config.data.sigma)\n",
    "    \n",
    "    net_cal = networks.MLP([64, 64, 64, 64]).to(device)\n",
    "    opt_cal = optim.Adam(net_cal.parameters(), lr=1e-4)\n",
    "    loss_cal = train_cal(producer, net_cal, opt_cal, config)\n",
    "    \n",
    "    net_ref = networks.Generator(input_dim=2, n_hidden=[64, 64, 64, 64], noise_dim=1).to(device)\n",
    "    opt_ref = optim.Adam(net_ref.parameters(), lr=1e-4)\n",
    "    net_dis = networks.Discriminator(input_dim=2, n_hidden=[64, 64, 64, 64]).to(device)\n",
    "    opt_dis = optim.Adam(net_dis.parameters(), lr=1e-4)\n",
    "    loss_dis, loss_ref, ll_ref = train_dis_ref(producer, net_cal, net_ref, opt_ref, net_dis, opt_dis, config)\n",
    "    metrics = {\"ll_ref\": ll_ref, \"loss_dis\": loss_dis, \"loss_ref\": loss_ref, \"loss_cal\": loss_cal}\n",
    "    return net_cal, net_ref, net_dis, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using the `Domain` object in the hypertunity framework we can easily iterate over all combinations of trainig and data configurations of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0db892c843f425a9588fd61bcc1f32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b28017e37c436b8f026769883820cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = ht.Domain({\"data\": {\"pi\": {0.5, 0.6, 0.9}, \n",
    "                              \"sigma\": {0.01, 0.02, 0.03}},\n",
    "                     \"model\": {\"lambda_cal\": {0, 1}, \"n_samples_cal\": {10}},\n",
    "                     \"train\": {\"batch_size_cal\": {1000}, \"iters_cal\": {1500},\n",
    "                               \"batch_size_adv\": {100}, \"iters_adv\": {5000}}\n",
    "                    })\n",
    "\n",
    "N_EXPERIMENTS = len(list(configs))\n",
    "N_REPS = 5\n",
    "\n",
    "results = [[] for i in range(N_EXPERIMENTS)]\n",
    "for i, cfg in tqdm.tqdm(enumerate(configs), total=N_EXPERIMENTS):\n",
    "    cfg = cfg.as_namedtuple()\n",
    "    for j in tqdm.tqdm(range(N_REPS), total=N_REPS, leave=False):\n",
    "        net_cal, net_ref, net_dis, metrics = build_and_train(cfg)\n",
    "        results[i].append((str(cfg), metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need some indexing operations to extract the loss curves and prepare for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [r for r in results if \"lambda_cal=0\" in r[0][0]]\n",
    "calibrated = [r for r in results if \"lambda_cal=1\" in r[0][0]]\n",
    "\n",
    "baseline_ll = np.asarray([[run[1][\"ll_ref\"] for run in runs] for runs in baseline])\n",
    "baseline_ll = baseline_ll.reshape(3, 3, N_REPS, -1).transpose(1, 0, 2, 3)\n",
    "\n",
    "calibrated_ll = np.asarray([[run[1][\"ll_ref\"] for run in runs] for runs in calibrated])\n",
    "calibrated_ll = calibrated_ll.reshape(3, 3, N_REPS, -1).transpose(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we visualise the average data log-likelihood of all experiments over the course training. We plot the median and Q1-Q3 quartiles to get a sense of the variance of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agg_loss_avg(baseline, calibrated, as_bundle=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "    N = baseline.shape[-1]\n",
    "    ax.grid(True)\n",
    "    total_experiments = N_EXPERIMENTS // 2 * N_REPS\n",
    "    median1 = np.median(baseline.reshape(total_experiments, -1), axis=0)\n",
    "    upper1 = np.quantile(baseline.reshape(total_experiments, -1), 0.75, axis=0)\n",
    "    lower1 = np.quantile(baseline.reshape(total_experiments, -1), 0.25, axis=0)\n",
    "    ax.fill_between(np.arange(N), lower1, upper1, facecolor=\"royalblue\", alpha=0.4, \n",
    "                          label=\"iqr (baseline)\")\n",
    "    ax.plot(np.arange(N), median1, \"-\", \n",
    "                   linewidth=1.5, color=\"navy\", label=\"median (baseline)\")\n",
    "    median2 = np.median(calibrated.reshape(total_experiments, -1), axis=0)\n",
    "    upper2 = np.quantile(calibrated.reshape(total_experiments, -1), 0.75, axis=0)\n",
    "    lower2 = np.quantile(calibrated.reshape(total_experiments, -1), 0.25, axis=0)\n",
    "    ax.fill_between(\n",
    "        np.arange(N), lower2, upper2, facecolor=\"coral\", alpha=0.6, \n",
    "        label=\"iqr (with calibration)\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        np.arange(N), median2, \"-\", linewidth=1.5, color=\"maroon\", \n",
    "        label=\"median (with calibration)\"\n",
    "    )\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_ylabel(f\"Average data log-likelihood\")\n",
    "    ax.set_xlabel(f\"Training iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_agg_loss_avg(baseline_ll, calibrated_ll, as_bundle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lower interquartile range and higher median line for the calibrated adversarial approach show that using the calibration target improves stability in training and increases convergence speed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower interquartile range and higher median line for the calibrated adversarial approach show that using the calibration target improves stability in training and increases convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}